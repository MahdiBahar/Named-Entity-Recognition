{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "840b2074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi/Named-Entity-Recognition/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-ner-uncased were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw ParsBERT‐NER output:\n",
      "  span='بانک ملت'   label=organization   score=0.975\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "MODEL_NAME = \"HooshvareLab/bert-base-parsbert-ner-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "sentence = \"این یک جمله تستی است که در آن باید بانک ملت و کد ملی تشخیص داده شود\"\n",
    "preds = ner_pipe(sentence)\n",
    "\n",
    "print(\"Raw ParsBERT‐NER output:\")\n",
    "for p in preds:\n",
    "    print(f\"  span='{p['word']}'   label={p['entity_group']}   score={p['score']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b9c9a7",
   "metadata": {},
   "source": [
    "## Read .CoNLL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5665c842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('-DOCSTART-', 'O'), ('در', 'O'), ('پردازش', 'B-ACTION'), ('717', 'O'), ('(', 'O'), ('تبادل', 'O'), ('مانده', 'O'), ('انتقالی', 'O'), ('کاربر', 'O'), (')', 'O'), ('پیغام', 'O'), ('خطای', 'B-ERROR'), ('\"', 'O'), ('\"', 'O'), ('کد', 'O'), ('ملی', 'O'), ('وارد', 'O'), ('شده', 'O'), ('مرتبط', 'O'), ('با', 'O'), ('این', 'O'), ('عملیات', 'O'), ('نمی', 'O'), ('باشد.', 'O'), ('\"', 'O'), ('\"', 'O'), ('دریافت', 'O'), ('می', 'O'), ('گردد.', 'O'), ('در', 'O'), ('تمامی', 'O'), ('فعالیتهایی', 'O'), ('که', 'O'), ('منجر', 'O'), ('به', 'O'), ('ایجاد', 'O'), ('مانده', 'B-ACTION'), ('انتقالی', 'B-ACTION'), ('می', 'O'), ('شوند،', 'O'), ('شناسه', 'B-CUSTOMER_ID'), ('مشتری', 'B-CUSTOMER_ID'), ('(', 'O'), ('کدملی', 'B-IDENTIFICATION_ID'), ('/', 'O'), ('شناسه', 'B-IDENTIFICATION_ID'), ('ملی', 'B-IDENTIFICATION_ID'), (')', 'O'), ('در', 'O'), ('فرم', 'O'), ('پولشویی', 'B-EVENT'), ('از', 'O'), ('اطلاعات', 'O'), ('قبلی', 'O'), ('بازیابی', 'O'), ('نشده', 'O'), ('و', 'O'), ('کاربر', 'O'), ('ملزم', 'O'), ('به', 'O'), ('درج', 'O'), ('شناسه', 'B-CUSTOMER_ID'), ('مشتری', 'B-CUSTOMER_ID'), ('(', 'O'), ('کدملی', 'B-IDENTIFICATION_ID'), ('/', 'O'), ('شناسه', 'B-IDENTIFICATION_ID'), ('ملی', 'B-IDENTIFICATION_ID'), (')', 'O'), ('در', 'O'), ('فرم', 'O'), ('پولشویی', 'B-EVENT'), ('برای', 'O'), ('فعالیت', 'O'), ('بعدی', 'O'), ('می', 'O'), ('باشد.', 'O'), ('پس', 'O'), ('از', 'O'), ('درج', 'O'), ('شناسه', 'O'), ('مشتری', 'O'), ('توسط', 'O'), ('کاربر،', 'O'), ('سامانه', 'O'), ('بانکداری', 'O'), ('متمرکز', 'O'), ('کنترل', 'O'), ('می', 'O'), ('نماید', 'O'), ('که', 'O'), ('شناسه', 'O'), ('مذکور', 'O'), ('با', 'O'), ('شناسه', 'O'), ('مانده', 'O'), ('انتقالی', 'O'), ('باز', 'O'), ('کاربر', 'O'), ('همخوانی', 'O'), ('داشته', 'O'), ('باشد.', 'O'), ('به', 'O'), ('عبارتی', 'O'), ('مانده', 'O'), ('انتقالی', 'O'), ('ایجاد', 'O'), ('شده', 'O'), ('صرفا', 'O'), ('برای', 'O'), ('یک', 'O'), ('شخص', 'O'), ('با', 'O'), ('یک', 'O'), ('شماره', 'O'), ('تبادل', 'O'), ('منحصر', 'O'), ('بفرد', 'O'), ('قابل', 'O'), ('پردازش', 'O'), ('می', 'O'), ('باشد', 'O'), ('.', 'O')], [('جهت', 'O'), ('استعلام', 'B-ACTION'), ('کدملی', 'B-IDENTIFICATION_ID'), ('مربوط', 'O'), ('به', 'O'), ('مانده', 'O'), ('ایجادی', 'O'), ('می', 'O'), ('بایست', 'O'), ('از', 'O'), ('طریق', 'O'), ('سامانه', 'B-FINANCIAL_PRODUCT'), ('بک', 'B-FINANCIAL_PRODUCT'), ('آفیس»', 'B-FINANCIAL_PRODUCT'), ('پشتیبانی', 'O'), ('فنی»', 'O'), ('بازیابی', 'B-ACTION'), ('اطلاعات', 'B-ACTION'), ('شناسه', 'O'), ('مشتری', 'O'), ('تسک', 'O'), ('انتقالی', 'O'), ('اقدام', 'O'), ('گردد', 'O'), ('.', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "def read_conll_file(file_path):\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 4:\n",
    "                    token, pos, chunk, ner = parts\n",
    "                elif len(parts) == 3:\n",
    "                    token, pos, ner = parts\n",
    "                    chunk = \"_\"  # placeholder if missing\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid line: {line}\")\n",
    "                current_sentence.append((token, ner))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Example usage:\n",
    "data = read_conll_file(\"Labeled_NER.conll\")\n",
    "print(data)  # Print first sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003c5af",
   "metadata": {},
   "source": [
    "## Generate fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e354e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "# Base examples (your list of sentences)\n",
    "base_data = data.copy() \n",
    "\n",
    "# Define replacements for augmentation\n",
    "synonyms = {\n",
    "    'استعلام': ['درخواست', 'بررسی', 'پرس‌وجو'],\n",
    "    'کدملی': ['شناسه ملی', 'کد شناسایی'],\n",
    "    'سامانه': ['سیستم', 'پلتفرم'],\n",
    "    'بازیابی': ['دریافت', 'بازخوانی']\n",
    "}\n",
    "\n",
    "def augment_sentence(sentence):\n",
    "    new_sentence = []\n",
    "    for word, label in sentence:\n",
    "        if word in synonyms:\n",
    "            new_word = random.choice(synonyms[word])\n",
    "        else:\n",
    "            new_word = word\n",
    "        new_sentence.append((new_word, label))\n",
    "    return new_sentence\n",
    "\n",
    "# Generate 10,000 examples\n",
    "augmented_data = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    base = random.choice(base_data)\n",
    "    augmented = augment_sentence(base)\n",
    "    augmented_data.append(augmented)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023839f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe1f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "997a6c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-ACTION', 'I-ACTION', 'B-IDENTIFICATION_ID', 'I-IDENTIFICATION_ID',\n",
    "              'B-FINANCIAL_PRODUCT', 'I-FINANCIAL_PRODUCT']\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6473b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw sentences into structured format\n",
    "def convert_to_hf_format(raw_sentences, label2id):\n",
    "    dataset = []\n",
    "    for sentence in raw_sentences:\n",
    "        tokens = [word for word, tag in sentence]\n",
    "        ner_tags = [label2id.get(tag, 0) for _, tag in sentence]  # default to 'O' if unknown\n",
    "        dataset.append({\n",
    "            \"tokens\": tokens,\n",
    "            \"ner_tags\": ner_tags\n",
    "        })\n",
    "    return dataset\n",
    "\n",
    "converted_dataset = convert_to_hf_format(data, label2id)\n",
    "converted_dataset = convert_to_hf_format(augmented_data, label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05c7e4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2db6995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "hf_dataset = Dataset.from_list(converted_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3623904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-ner-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# def tokenize_and_align_labels(examples):\n",
    "#     tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    \n",
    "#     labels = []\n",
    "#     for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "#         word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "#         previous_word_idx = None\n",
    "#         label_ids = []\n",
    "#         for word_idx in word_ids:\n",
    "#             if word_idx is None:\n",
    "#                 label_ids.append(-100)  # ignored in loss\n",
    "#             elif word_idx != previous_word_idx:\n",
    "#                 label_ids.append(label[word_idx])  # B or O\n",
    "#             else:\n",
    "#                 label_ids.append(label[word_idx] if label[word_idx] % 2 == 1 else label[word_idx] + 1)  # I if needed\n",
    "#             previous_word_idx = word_idx\n",
    "#         labels.append(label_ids)\n",
    "\n",
    "#     tokenized_inputs[\"labels\"] = labels\n",
    "#     return tokenized_inputs\n",
    "\n",
    "# tokenized_dataset = hf_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        is_split_into_words=True,\n",
    "        return_offsets_mapping=True  # helps alignment\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # For subwords, use I- version if available\n",
    "                label_ids.append(label[word_idx] if label[word_idx] % 2 == 1 else label[word_idx] + 1)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4b8b193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 3377.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = hf_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"tokens\", \"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ab9cdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-ner-uncased were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-ner-uncased and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21, 768]) in the checkpoint and torch.Size([7, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=len(label_list),\n",
    "#     id2label=id2label,\n",
    "#     label2id=label2id\n",
    "# )\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True  # ✅ this line solves missmatch problem\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "795626aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.03169589742024739, metrics={'train_runtime': 36.2883, 'train_samples_per_second': 82.671, 'train_steps_per_second': 10.334, 'total_flos': 209761370478000.0, 'train_loss': 0.03169589742024739, 'epoch': 3.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.data.data_collator import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator  # ✅ required for NER!\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b68f6cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 137\n"
     ]
    }
   ],
   "source": [
    "sample = tokenized_dataset[0]\n",
    "print(len(sample['input_ids']), len(sample['labels']))  # These should match!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "909be7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./ner_model_fakedata/tokenizer_config.json',\n",
       " './ner_model_fakedata/special_tokens_map.json',\n",
       " './ner_model_fakedata/vocab.txt',\n",
       " './ner_model_fakedata/added_tokens.json',\n",
       " './ner_model_fakedata/tokenizer.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./ner_model_fakedata\")\n",
    "tokenizer.save_pretrained(\"./ner_model_fakedata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d6382",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b0ce371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "# from transformers.pipelines import pipeline\n",
    "\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"./ner_model\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./ner_model\")\n",
    "# ner_pipe = pipeline(\n",
    "#     \"ner\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     aggregation_strategy=\"simple\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2391ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test it\n",
    "# text = \"اطلاعات مشتری باید از طریق سامانه بک آفیس بازیابی گردد.\"\n",
    "# try:\n",
    "#     result = ner_pipe(text)\n",
    "#     if result:\n",
    "#         for entity in result:\n",
    "#             if isinstance(entity, dict):\n",
    "#                 print(f\"{entity['word']} -> {entity['entity_group']} (score={entity['score']:.2f})\")\n",
    "#             else:\n",
    "#                 print(f\"Unexpected entity format: {entity}\")\n",
    "#     else:\n",
    "#         print(\"No entities were found in the text.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error processing text: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cf773e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           → O\n",
      "در              → O\n",
      "پردازش          → B-ACTION\n",
      "[UNK]           → O\n",
      "(               → O\n",
      "تبادل           → O\n",
      "مانده           → O\n",
      "انتقالی         → O\n",
      "کاربر           → O\n",
      ")               → O\n",
      "پیغام           → O\n",
      "خطای            → O\n",
      "کد              → O\n",
      "ملی             → O\n",
      "وارد            → O\n",
      "شده             → O\n",
      "مرتبط           → O\n",
      "با              → O\n",
      "این             → O\n",
      "عملیات          → O\n",
      "نمی             → O\n",
      "باشد            → O\n",
      ".               → B-ACTION\n",
      "دریافت          → O\n",
      "می              → O\n",
      "گردد            → O\n",
      ".               → B-ACTION\n",
      "در              → O\n",
      "تمامی           → O\n",
      "فعالیتهایی      → O\n",
      "که              → O\n",
      "منجر            → O\n",
      "به              → O\n",
      "ایجاد           → O\n",
      "مانده           → B-ACTION\n",
      "انتقالی         → B-ACTION\n",
      "می              → O\n",
      "شوند            → O\n",
      "،               → B-ACTION\n",
      "شناسه           → O\n",
      "مشتری           → O\n",
      "(               → O\n",
      "کدملی           → O\n",
      "/               → O\n",
      "شناسه           → B-IDENTIFICATION_ID\n",
      "ملی             → B-IDENTIFICATION_ID\n",
      ")               → O\n",
      "در              → O\n",
      "فرم             → O\n",
      "پولشویی         → O\n",
      "از              → O\n",
      "اطلاعات         → O\n",
      "قبلی            → O\n",
      "بازیابی         → O\n",
      "نشده            → O\n",
      "و               → O\n",
      "کاربر           → O\n",
      "ملزم            → O\n",
      "به              → O\n",
      "درج             → O\n",
      "شناسه           → O\n",
      "مشتری           → O\n",
      "(               → O\n",
      "کدملی           → O\n",
      "/               → O\n",
      "شناسه           → B-IDENTIFICATION_ID\n",
      "ملی             → B-IDENTIFICATION_ID\n",
      ")               → O\n",
      "در              → O\n",
      "فرم             → O\n",
      "پولشویی         → O\n",
      "برای            → O\n",
      "فعالیت          → O\n",
      "بعدی            → O\n",
      "می              → O\n",
      "باشد            → O\n",
      ".               → B-ACTION\n",
      "پس              → O\n",
      "از              → O\n",
      "درج             → O\n",
      "شناسه           → O\n",
      "مشتری           → O\n",
      "توسط            → O\n",
      "کاربر           → O\n",
      "،               → B-ACTION\n",
      "سامانه          → O\n",
      "بانکداری        → O\n",
      "متمرکز          → O\n",
      "کنترل           → O\n",
      "می              → O\n",
      "نماید           → O\n",
      "که              → O\n",
      "شناسه           → O\n",
      "مذکور           → O\n",
      "با              → O\n",
      "شناسه           → O\n",
      "مانده           → O\n",
      "انتقالی         → O\n",
      "باز             → O\n",
      "کاربر           → O\n",
      "همخوانی         → O\n",
      "داشته           → O\n",
      "باشد            → O\n",
      ".               → B-ACTION\n",
      "به              → O\n",
      "عبارتی          → O\n",
      "مانده           → O\n",
      "انتقالی         → O\n",
      "ایجاد           → O\n",
      "شده             → O\n",
      "صرفا            → O\n",
      "برای            → O\n",
      "یک              → O\n",
      "شخص             → O\n",
      "با              → O\n",
      "یک              → O\n",
      "شماره           → O\n",
      "تبادل           → O\n",
      "منحصر           → O\n",
      "بفرد            → O\n",
      "قابل            → O\n",
      "پردازش          → O\n",
      "می              → O\n",
      "باشد            → O\n",
      ".               → O\n",
      "[SEP]           → O\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./ner_model_fakedata\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./ner_model_fakedata\")\n",
    "\n",
    "text = \"اطلاعات مشتری باید از طریق سامانه بک آفیس بازیابی گردد در بانک ملت.\"\n",
    "text = \"اطلاعات مشتری در بانک مرکزی و بانک ملت باید از طریق سامانه بک آفیس بازیابی گردد.\"\n",
    "text = \"در پردازش 717 (تبادل مانده انتقالی کاربر) پیغام خطای \"\"کد ملی وارد شده مرتبط با این عملیات نمی باشد.\"\" دریافت می گردد. در تمامی فعالیتهایی که منجر به ایجاد مانده انتقالی می شوند، شناسه مشتری (کدملی/ شناسه ملی) در فرم پولشویی از اطلاعات قبلی بازیابی نشده و کاربر ملزم به درج شناسه مشتری (کدملی/ شناسه ملی) در فرم پولشویی برای فعالیت بعدی می باشد. پس از درج شناسه مشتری توسط کاربر، سامانه بانکداری متمرکز کنترل می نماید که شناسه مذکور با شناسه مانده انتقالی باز کاربر همخوانی داشته باشد. به عبارتی مانده انتقالی ایجاد شده صرفا برای یک شخص با یک شماره تبادل منحصر بفرد قابل پردازش می باشد.\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "for token, pred_id in zip(tokens, predictions[0]):\n",
    "    print(f\"{token:15} → {model.config.id2label[pred_id.item()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "055ae368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-ACTION',\n",
       " 2: 'I-ACTION',\n",
       " 3: 'B-IDENTIFICATION_ID',\n",
       " 4: 'I-IDENTIFICATION_ID',\n",
       " 5: 'B-FINANCIAL_PRODUCT',\n",
       " 6: 'I-FINANCIAL_PRODUCT'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8789485d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
